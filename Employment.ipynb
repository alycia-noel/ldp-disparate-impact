{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c55d95b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ba09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import xxhash\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, log_loss, accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sys import maxsize\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from folktables import ACSDataSource, ACSIncome, ACSPublicCoverage, ACSEmployment\n",
    "\n",
    "from copy import deepcopy\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2489ee",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c937515",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "ca_data = data_source.get_data(states=[\"CA\"], download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9877e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ACSEmployment():\n",
    "    df, labels, _ = ACSEmployment.df_to_pandas(ca_data)\n",
    "    \n",
    "    df = pd.concat([df, labels], axis=1)\n",
    "\n",
    "    df = df.drop(['DREM', 'DEAR', 'DEYE', 'RELP', 'MAR', 'ESP', 'CIT', 'MIG', 'ANC', 'MIL'], axis=1)\n",
    "    df= df.drop_duplicates(keep='first', ignore_index=True)\n",
    "    df = df.dropna(how='any', axis=0)\n",
    "    \n",
    "    df['AGEP'] = np.where(df['AGEP'] >= df['AGEP'].mean(), 1, 0)\n",
    "    df['SCHL'] = np.where(df['SCHL'] >= df['SCHL'].mean(), 1, 0)\n",
    "    \n",
    "    def binarize(dataset, features):\n",
    "        dataset[features] = np.where(df[features] == 1, 1, 0)\n",
    "    \n",
    "    binarize(df, ['SEX', 'DIS', 'RAC1P', 'NATIVITY', 'ESR'])\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype('int')\n",
    "    \n",
    "    num_train = int(len(df) * .8)\n",
    "    dfTrain = df.sample(n=num_train, replace=False, axis=0, ignore_index=False)\n",
    "    dfTest = df.drop(dfTrain.index, axis=0)\n",
    "\n",
    "    return dfTrain, dfTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c51dee",
   "metadata": {},
   "source": [
    "### Randomized Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRR_Client(input_data, p):\n",
    "    \n",
    "    if np.random.binomial(1, p) == 1:\n",
    "        return input_data\n",
    "\n",
    "    else:\n",
    "        return 1 - input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0116489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_keys(num_feat):\n",
    "    total = 2 ** (num_feat+1)\n",
    "    possible_keys = ['0' for _ in range(int(total/2))]\n",
    "    possible_keys.extend(['1' for _ in range(int(total/2))])\n",
    "    \n",
    "    rounds = [i+1 for i in range(num_feat)]\n",
    "    \n",
    "    for r in rounds[::-1]:\n",
    "        count = 0\n",
    "        for i, k in enumerate(possible_keys):\n",
    "            if count < 2**(r-1):\n",
    "                possible_keys[i] = k + '0'\n",
    "            else:\n",
    "                possible_keys[i] = k +'1'\n",
    "            count += 1\n",
    "            if count == 2**r:\n",
    "                count = 0\n",
    "                \n",
    "    return possible_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb543ac0",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868254ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main(epsilon, which_set):\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"EPSILON: \" + str(epsilon))\n",
    "    print(\"=======================================================\")\n",
    "    \n",
    "    w_all_acc_lr, nw_all_acc_lr = [], []\n",
    "    w_all_acc_nb, nw_all_acc_nb = [], []\n",
    "    w_all_acc_lgbm, nw_all_acc_lgbm = [], []\n",
    "\n",
    "    for itr in range(100):\n",
    "        \n",
    "        if itr % 10 == 0 and itr != 0:\n",
    "            print(str(itr)+'/100')\n",
    "            \n",
    "        dfTrain_main, dfTest_main = get_ACSEmployment()\n",
    "\n",
    "        dfTrain = deepcopy(dfTrain_main)\n",
    "        dfTest = deepcopy(dfTest_main)\n",
    "\n",
    "        X_test = dfTest.loc[:, dfTest.columns != 'ESR']\n",
    "        y_test = dfTest.loc[:, 'ESR']\n",
    "\n",
    "        w_loc, w_true = [], []\n",
    "        nw_true = []\n",
    "\n",
    "        for i, (index, r) in enumerate(X_test.iterrows()):\n",
    "            if r['RAC1P'] == 1: # 1- white\n",
    "                w_loc.append(i)\n",
    "                w_true.append(y_test.loc[index])\n",
    "            else:\n",
    "                nw_true.append(y_test.loc[index])\n",
    "\n",
    "        X_train = dfTrain.loc[:, dfTrain.columns != 'ESR']\n",
    "        y_train = dfTrain.loc[:, 'ESR']\n",
    "        \n",
    "        # Do randomized response\n",
    "        if epsilon:\n",
    "            p = np.exp(epsilon) / (np.exp(epsilon) + 1)\n",
    "\n",
    "            lst_df_train = []\n",
    "            lst_df_test = []\n",
    "            sensitive_att = ['AGEP', 'SEX', 'RAC1P']\n",
    "\n",
    "            if which_set in ['feat-lab', 'feat']:\n",
    "                for col in list(set(X_train.columns)):\n",
    "                    if col in sensitive_att:\n",
    "                        df_new_col = pd.DataFrame([int(GRR_Client(val, p)) for val in X_train[col]], columns=[col])\n",
    "                        lst_df_train.append(df_new_col)\n",
    "                    else:\n",
    "                        lst_df_train.append(pd.DataFrame([int(val) for val in X_train[col]], columns=[col]))\n",
    "\n",
    "                X_train = pd.concat(lst_df_train, axis=1)\n",
    "\n",
    "                if which_set == 'feat-lab':\n",
    "                    y_train = pd.DataFrame([int(GRR_Client(val, p)) for val in y_train], columns=['ESR'])\n",
    "                else:\n",
    "                    y_train = pd.DataFrame([int(val) for val in y_train], columns=['ESR'])\n",
    "                    \n",
    "            elif which_set == 'lab':\n",
    "                for col in list(set(X_train.columns)):\n",
    "                    lst_df_train.append(pd.DataFrame([int(val) for val in X_train[col]], columns=[col]))\n",
    "                X_train = pd.concat(lst_df_train, axis=1)\n",
    "                \n",
    "                y_train = pd.DataFrame([int(GRR_Client(val, p)) for val in y_train], columns=['ESR'])\n",
    "                \n",
    "            # perform reconstruction\n",
    "            num_repeat = len(X_train.columns)\n",
    "\n",
    "            possible_keys =  gen_keys(num_repeat)\n",
    "            lambda_dict = {}\n",
    "\n",
    "            for key in possible_keys:\n",
    "                lambda_dict[key] = 0\n",
    "\n",
    "            joint_train = pd.concat([X_train, y_train], axis=1)\n",
    "         \n",
    "            for index, row in joint_train.iterrows():\n",
    "                key = ''.join(str(x) for x in row)\n",
    "                lambda_dict[key] += 1\n",
    "            \n",
    "            if which_set == 'feat':\n",
    "                selected_n = [joint_train.columns.get_loc(sele_feat) for sele_feat in sensitive_att]\n",
    "            elif which_set == 'feat-lab':\n",
    "                selected_n = [joint_train.columns.get_loc(sele_feat) for sele_feat in sensitive_att]\n",
    "                selected_n.append(num_repeat)\n",
    "            else:\n",
    "                selected_n = [num_repeat]\n",
    "        \n",
    "            if 0 in selected_n:\n",
    "                p_ = np.linalg.inv([[p, 1-p],[1-p, p]])\n",
    "            else:\n",
    "                p_ = np.linalg.inv([[1, 0],[0, 1]])\n",
    "\n",
    "            # get P^-1\n",
    "            for n in range(num_repeat+1):\n",
    "                if n == 0:\n",
    "                    continue\n",
    "                if n in selected_n:\n",
    "                    b = np.linalg.inv([[p, 1-p], [1-p, p]])\n",
    "                    p_ = np.kron(p_, b)\n",
    "                else:\n",
    "                    b  = np.linalg.inv([[1,0], [0,1]])\n",
    "                    p_ = np.kron(p_, b)\n",
    "       \n",
    "            # construct big lambda in order\n",
    "            keys = list(lambda_dict.keys())\n",
    "            keys.sort()\n",
    "            sorted_lambda_dict = {i: lambda_dict[i] for i in keys}\n",
    "           \n",
    "      \n",
    "            lambda_list = [lambda_dict[k]/len(X_train) for k in keys] #lambda hat \n",
    "            \n",
    "            pi_tilde = np.matmul(p_, lambda_list)\n",
    "            \n",
    "            for i, pi in enumerate(pi_tilde):\n",
    "                if pi < 0:\n",
    "                    pi_tilde[i] = 0\n",
    "\n",
    "            pi_tilde_scaled = np.true_divide(pi_tilde, np.sum(pi_tilde))\n",
    "\n",
    "            pi_tilde_list = [round(pi*len(X_train)) for pi in pi_tilde_scaled]\n",
    "\n",
    "            recon_train = []\n",
    "\n",
    "            for i, counts in enumerate(pi_tilde_list):\n",
    "                for j in range(counts):\n",
    "                    recon_train.append([int(elem) for elem in keys[i]])\n",
    "\n",
    "            recon_train = pd.DataFrame(recon_train, columns=dfTrain.columns)\n",
    "            recon_train = recon_train.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            X_train, y_train = recon_train.loc[:, dfTrain.columns != 'ESR'], recon_train.loc[:, 'ESR']\n",
    "            \n",
    "        #######################\n",
    "        # Logistic Regression #\n",
    "        #######################\n",
    "        LR = LogisticRegression(max_iter=500, fit_intercept=True)\n",
    "        LR.fit(X_train, y_train)\n",
    "        predictions1 = LR.predict(X_test)\n",
    "        w_pred1, nw_pred1 = [], []\n",
    "\n",
    "        for i, p in enumerate(predictions1):\n",
    "            if i in w_loc:\n",
    "                w_pred1.append(p)\n",
    "            else:\n",
    "                nw_pred1.append(p)\n",
    "\n",
    "        w_acc1 = accuracy_score(w_true, w_pred1)\n",
    "        nw_acc1 = accuracy_score(nw_true, nw_pred1)\n",
    "\n",
    "        w_all_acc_lr.append(w_acc1)\n",
    "        nw_all_acc_lr.append(nw_acc1)\n",
    "\n",
    "        ###############\n",
    "        # Naive Bayes #\n",
    "        ###############\n",
    "        NB = GaussianNB()\n",
    "        NB.fit(X_train, y_train)\n",
    "        predictions2 = NB.predict(X_test)\n",
    "        w_pred2, nw_pred2 = [], []\n",
    "\n",
    "        for i, p in enumerate(predictions2):\n",
    "            if i in w_loc:\n",
    "                w_pred2.append(p)\n",
    "            else:\n",
    "                nw_pred2.append(p)\n",
    "\n",
    "        w_acc2 = accuracy_score(w_true, w_pred2)\n",
    "        nw_acc2 = accuracy_score(nw_true, nw_pred2)\n",
    "\n",
    "        w_all_acc_nb.append(w_acc2)\n",
    "        nw_all_acc_nb.append(nw_acc2)\n",
    "\n",
    "        ########\n",
    "        # LGBM #\n",
    "        ########\n",
    "        LGBM = LGBMClassifier(verbose=-1)\n",
    "        LGBM.fit(X_train, y_train)\n",
    "        predictions3 = LGBM.predict(X_test)\n",
    "        w_pred3, nw_pred3 = [], []\n",
    "\n",
    "        for i, p in enumerate(predictions3):\n",
    "            if i in w_loc:\n",
    "                w_pred3.append(p)\n",
    "            else:\n",
    "                nw_pred3.append(p)\n",
    "\n",
    "        w_acc3 = accuracy_score(w_true, w_pred3)\n",
    "        nw_acc3 = accuracy_score(nw_true, nw_pred3)\n",
    "\n",
    "        w_all_acc_lgbm.append(w_acc3)\n",
    "        nw_all_acc_lgbm.append(nw_acc3)\n",
    "\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"AVERAGE\")\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"Logisitc Regression Model:\")\n",
    "    print(\"-- White Accuracy: \" + str(round(sum(w_all_acc_lr)/100,3)))\n",
    "    print(\"-- Non-white Accuracy: \" + str(round(sum(nw_all_acc_lr)/100,3)))\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"Naive Bayes Model:\")\n",
    "    print(\"-- White Accuracy: \" + str(round(sum(w_all_acc_nb)/100,3)))\n",
    "    print(\"-- Non-white Accuracy: \" + str(round(sum(nw_all_acc_nb)/100,3)))\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"LGBM Model:\")\n",
    "    print(\"-- White Accuracy: \" + str(round(sum(w_all_acc_lgbm)/100,3)))\n",
    "    print(\"-- Non-white Accuracy: \" + str(round(sum(nw_all_acc_lgbm)/100,3)))\n",
    "    print(\"=======================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a230c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epsilons = [.001, .01, .1, .25, .5, 1, 2, 5] # None gives original accuracy on non-rr data\n",
    "which_set = 'feat' # 'feat-lab', 'feat', 'lab'\n",
    "\n",
    "for e in epsilons:\n",
    "    main(e, which_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabfb979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
