{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c55d95b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920ba09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import xxhash\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, log_loss, accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sys import maxsize\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from folktables import ACSDataSource, ACSIncome, ACSPublicCoverage, ACSEmployment\n",
    "\n",
    "from copy import deepcopy\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2489ee",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c937515",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "ca_data = data_source.get_data(states=[\"CA\"], download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927e4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ACSIncome():\n",
    "    df, labels, _ = ACSIncome.df_to_pandas(ca_data)\n",
    "    df = pd.concat([df, labels], axis=1)\n",
    "    df= df.drop_duplicates(keep='first', ignore_index=True)\n",
    "    df = df.drop(['OCCP','POBP', 'RELP', 'MAR', 'COW'], axis=1)\n",
    "    df = df.dropna(how='any', axis=0)\n",
    "  \n",
    "    df['AGEP'] = np.where(df['AGEP'] >= df['AGEP'].mean(), 1, 0)\n",
    "    df['SCHL'] = np.where(df['SCHL'] >= df['SCHL'].mean(), 1, 0)\n",
    "    df['WKHP'] = np.where(df['WKHP'] >= df['WKHP'].mean(), 1, 0)\n",
    "    \n",
    "    df['SEX'] = df['SEX'].replace([1, 2], [1, 0]).astype('int')\n",
    "    df['RAC1P'] = np.where(df['RAC1P'] == 1, 1.0, 0.0).astype('int')\n",
    "    df['PINCP'] = df['PINCP'].replace([True, False], [1, 0])\n",
    "\n",
    "    num_train = int(len(df) * .8)\n",
    "    dfTrain = df.sample(n=num_train, replace=False, axis=0, ignore_index=False)\n",
    "    dfTest = df.drop(dfTrain.index, axis=0)\n",
    "    \n",
    "    return dfTrain, dfTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c51dee",
   "metadata": {},
   "source": [
    "### Randomized Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "647e2b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRR_Client(input_data, p):\n",
    "    \n",
    "    if np.random.binomial(1, p) == 1:\n",
    "        return input_data\n",
    "\n",
    "    else:\n",
    "        return 1 - input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e30e9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_keys(num_feat):\n",
    "    total = 2 ** (num_feat+1)\n",
    "    possible_keys = ['0' for _ in range(int(total/2))]\n",
    "    possible_keys.extend(['1' for _ in range(int(total/2))])\n",
    "    \n",
    "    rounds = [i+1 for i in range(num_feat)]\n",
    "    \n",
    "    for r in rounds[::-1]:\n",
    "        count = 0\n",
    "        for i, k in enumerate(possible_keys):\n",
    "            if count < 2**(r-1):\n",
    "                possible_keys[i] = k + '0'\n",
    "            else:\n",
    "                possible_keys[i] = k +'1'\n",
    "            count += 1\n",
    "            if count == 2**r:\n",
    "                count = 0\n",
    "                \n",
    "    return possible_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751a26a",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "868254ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main(epsilon, which_set):\n",
    "\n",
    "    m_all_acc_lr, f_all_acc_lr = [], []\n",
    "    m_all_acc_nb, f_all_acc_nb = [], []\n",
    "    m_all_acc_lgbm, f_all_acc_lgbm = [], []\n",
    "\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"EPSILON: \" + str(epsilon))\n",
    "    print(\"=======================================================\")\n",
    "        \n",
    "    for itr in range(100):\n",
    "        if itr % 10 == 0 and itr != 0:\n",
    "            print(str(itr)+'/100')\n",
    "        dfTrain_main, dfTest_main = get_ACSIncome()\n",
    "        \n",
    "        dfTrain = deepcopy(dfTrain_main)\n",
    "        dfTest = deepcopy(dfTest_main)\n",
    "\n",
    "        X_test = dfTest.loc[:, dfTest.columns != 'PINCP']\n",
    "        y_test = dfTest.loc[:, 'PINCP']\n",
    "\n",
    "        m_loc, m_true = [], []\n",
    "        f_true = []\n",
    "\n",
    "        for i, (index, r) in enumerate(X_test.iterrows()):\n",
    "            if r['SEX'] == 1:\n",
    "                m_loc.append(i)\n",
    "                m_true.append(y_test.loc[index])\n",
    "            else:\n",
    "                f_true.append(y_test.loc[index])\n",
    "\n",
    "        X_train = dfTrain.loc[:, dfTrain.columns != 'PINCP']\n",
    "        y_train = dfTrain.loc[:, 'PINCP']\n",
    "        \n",
    "        # Do randomized response\n",
    "        if epsilon:\n",
    "            p = np.exp(epsilon) / (np.exp(epsilon) + 1)\n",
    "\n",
    "            lst_df_train = []\n",
    "            lst_df_test = []\n",
    "            sensitive_att = ['AGEP', 'SEX', 'RAC1P']\n",
    "\n",
    "            if which_set in ['feat-lab', 'feat']:\n",
    "                for col in list(set(X_train.columns)):\n",
    "                    if col in sensitive_att:\n",
    "                        df_new_col = pd.DataFrame([int(GRR_Client(val, p)) for val in X_train[col]], columns=[col])\n",
    "                        lst_df_train.append(df_new_col)\n",
    "                    else:\n",
    "                        lst_df_train.append(pd.DataFrame([int(val) for val in X_train[col]], columns=[col]))\n",
    "\n",
    "                X_train = pd.concat(lst_df_train, axis=1)\n",
    "\n",
    "                if which_set == 'feat-lab':\n",
    "                    y_train = pd.DataFrame([int(GRR_Client(val, p)) for val in y_train], columns=['PINCP'])\n",
    "                else:\n",
    "                    y_train = pd.DataFrame([int(val) for val in y_train], columns=['PINCP'])\n",
    "                    \n",
    "            elif which_set == 'lab':\n",
    "                for col in list(set(X_train.columns)):\n",
    "                    lst_df_train.append(pd.DataFrame([int(val) for val in X_train[col]], columns=[col]))\n",
    "                X_train = pd.concat(lst_df_train, axis=1)\n",
    "                \n",
    "                y_train = pd.DataFrame([int(GRR_Client(val, p)) for val in y_train], columns=['PINCP'])\n",
    "            \n",
    "            # perform reconstruction\n",
    "            num_repeat = len(X_train.columns)\n",
    "\n",
    "            possible_keys =  gen_keys(num_repeat)\n",
    "            lambda_dict = {}\n",
    "\n",
    "            for key in possible_keys:\n",
    "                lambda_dict[key] = 0\n",
    "\n",
    "            joint_train = pd.concat([X_train, y_train], axis=1)\n",
    "         \n",
    "            for index, row in joint_train.iterrows():\n",
    "  \n",
    "                key = ''.join(str(x) for x in row)\n",
    "                lambda_dict[key] += 1\n",
    "            \n",
    "            if which_set == 'feat':\n",
    "                selected_n = [joint_train.columns.get_loc(sele_feat) for sele_feat in sensitive_att]\n",
    "            elif which_set == 'feat-lab':\n",
    "                selected_n = [joint_train.columns.get_loc(sele_feat) for sele_feat in sensitive_att]\n",
    "                selected_n.append(num_repeat)\n",
    "            else:\n",
    "                selected_n = [num_repeat]\n",
    "        \n",
    "            if 0 in selected_n:\n",
    "                p_ = np.linalg.inv([[p, 1-p],[1-p, p]])\n",
    "            else:\n",
    "                p_ = np.linalg.inv([[1, 0],[0, 1]])\n",
    "\n",
    "            # get P^-1\n",
    "            for n in range(num_repeat+1):\n",
    "                if n == 0:\n",
    "                    continue\n",
    "                if n in selected_n:\n",
    "                    b = np.linalg.inv([[p, 1-p], [1-p, p]])\n",
    "                    p_ = np.kron(p_, b)\n",
    "                else:\n",
    "                    b  = np.linalg.inv([[1,0], [0,1]])\n",
    "                    p_ = np.kron(p_, b)\n",
    "       \n",
    "            # construct big lambda in order\n",
    "            keys = list(lambda_dict.keys())\n",
    "            keys.sort()\n",
    "            sorted_lambda_dict = {i: lambda_dict[i] for i in keys}\n",
    "           \n",
    "      \n",
    "            lambda_list = [lambda_dict[k]/len(X_train) for k in keys] #lambda hat \n",
    "            \n",
    "            pi_tilde = np.matmul(p_, lambda_list)\n",
    "            \n",
    "            for i, pi in enumerate(pi_tilde):\n",
    "                if pi < 0:\n",
    "                    pi_tilde[i] = 0\n",
    "\n",
    "            pi_tilde_scaled = np.true_divide(pi_tilde, np.sum(pi_tilde))\n",
    "\n",
    "            pi_tilde_list = [round(pi*len(X_train)) for pi in pi_tilde_scaled]\n",
    "\n",
    "            recon_train = []\n",
    "\n",
    "            for i, counts in enumerate(pi_tilde_list):\n",
    "                for j in range(counts):\n",
    "                    recon_train.append([int(elem) for elem in keys[i]])\n",
    "\n",
    "            recon_train = pd.DataFrame(recon_train, columns=dfTrain.columns)\n",
    "            recon_train = recon_train.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            X_train, y_train = recon_train.loc[:, dfTrain.columns != 'PINCP'], recon_train.loc[:, 'PINCP']\n",
    "            \n",
    "        #######################\n",
    "        # Logistic Regression #\n",
    "        #######################\n",
    "        LR = LogisticRegression(max_iter=500, fit_intercept=True)\n",
    "        LR.fit(X_train, y_train)\n",
    "        predictions1 = LR.predict(X_test)\n",
    "        m_pred1, f_pred1 = [], []\n",
    "\n",
    "        for i, p in enumerate(predictions1):\n",
    "            if i in m_loc:\n",
    "                m_pred1.append(p)\n",
    "            else:\n",
    "                f_pred1.append(p)\n",
    "\n",
    "        m_acc1 = accuracy_score(m_true, m_pred1)\n",
    "        f_acc1 = accuracy_score(f_true, f_pred1)\n",
    "\n",
    "        m_all_acc_lr.append(m_acc1)\n",
    "        f_all_acc_lr.append(f_acc1)\n",
    "\n",
    "        ###############\n",
    "        # Naive Bayes #\n",
    "        ###############\n",
    "        NB = GaussianNB()\n",
    "        NB.fit(X_train, y_train)\n",
    "        predictions2 = NB.predict(X_test)\n",
    "        m_pred2, f_pred2 = [], []\n",
    "\n",
    "        for i, p in enumerate(predictions2):\n",
    "            if i in m_loc:\n",
    "                m_pred2.append(p)\n",
    "            else:\n",
    "                f_pred2.append(p)\n",
    "\n",
    "        m_acc2 = accuracy_score(m_true, m_pred2)\n",
    "        f_acc2 = accuracy_score(f_true, f_pred2)\n",
    "\n",
    "        m_all_acc_nb.append(m_acc2)\n",
    "        f_all_acc_nb.append(f_acc2)\n",
    "\n",
    "        ########\n",
    "        # LGBM #\n",
    "        ########\n",
    "        LGBM = LGBMClassifier(verbose=-1)\n",
    "        LGBM.fit(X_train, y_train)\n",
    "        predictions3 = LGBM.predict(X_test)\n",
    "        m_pred3, f_pred3 = [], []\n",
    "\n",
    "        for i, p in enumerate(predictions3):\n",
    "            if i in m_loc:\n",
    "                m_pred3.append(p)\n",
    "            else:\n",
    "                f_pred3.append(p)\n",
    "\n",
    "        m_acc3 = accuracy_score(m_true, m_pred3)\n",
    "        f_acc3 = accuracy_score(f_true, f_pred3)\n",
    "\n",
    "        m_all_acc_lgbm.append(m_acc3)\n",
    "        f_all_acc_lgbm.append(f_acc3)\n",
    "\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"AVERAGE\")\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"Logisitc Regression Model:\")\n",
    "    print(\"-- Male Accuracy: \" + str(round(sum(m_all_acc_lr)/100,3)))\n",
    "    print(\"-- Female Accuracy: \" + str(round(sum(f_all_acc_lr)/100,3)))\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"Naive Bayes Model:\")\n",
    "    print(\"-- Male Accuracy: \" + str(round(sum(m_all_acc_nb)/100,3)))\n",
    "    print(\"-- Female Accuracy: \" + str(round(sum(f_all_acc_nb)/100,3)))\n",
    "    print(\"=======================================================\")\n",
    "    print(f\"LGBM Model:\")\n",
    "    print(\"-- Male Accuracy: \" + str(round(sum(m_all_acc_lgbm)/100,3)))\n",
    "    print(\"-- Female Accuracy: \" + str(round(sum(f_all_acc_lgbm)/100,3)))\n",
    "    print(\"=======================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81395b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "EPSILON: 0.001\n",
      "=======================================================\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "=======================================================\n",
      "AVERAGE\n",
      "=======================================================\n",
      "Logisitc Regression Model:\n",
      "-- Male Accuracy: 0.506\n",
      "-- Female Accuracy: 0.512\n",
      "=======================================================\n",
      "Naive Bayes Model:\n",
      "-- Male Accuracy: 0.503\n",
      "-- Female Accuracy: 0.51\n",
      "=======================================================\n",
      "LGBM Model:\n",
      "-- Male Accuracy: 0.507\n",
      "-- Female Accuracy: 0.501\n",
      "=======================================================\n",
      "=======================================================\n",
      "EPSILON: 0.01\n",
      "=======================================================\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "=======================================================\n",
      "AVERAGE\n",
      "=======================================================\n",
      "Logisitc Regression Model:\n",
      "-- Male Accuracy: 0.552\n",
      "-- Female Accuracy: 0.575\n",
      "=======================================================\n",
      "Naive Bayes Model:\n",
      "-- Male Accuracy: 0.56\n",
      "-- Female Accuracy: 0.581\n",
      "=======================================================\n",
      "LGBM Model:\n",
      "-- Male Accuracy: 0.545\n",
      "-- Female Accuracy: 0.524\n",
      "=======================================================\n",
      "=======================================================\n",
      "EPSILON: 0.1\n",
      "=======================================================\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "=======================================================\n",
      "AVERAGE\n",
      "=======================================================\n",
      "Logisitc Regression Model:\n",
      "-- Male Accuracy: 0.673\n",
      "-- Female Accuracy: 0.661\n",
      "=======================================================\n",
      "Naive Bayes Model:\n",
      "-- Male Accuracy: 0.674\n",
      "-- Female Accuracy: 0.664\n",
      "=======================================================\n",
      "LGBM Model:\n",
      "-- Male Accuracy: 0.657\n",
      "-- Female Accuracy: 0.655\n",
      "=======================================================\n",
      "=======================================================\n",
      "EPSILON: 0.25\n",
      "=======================================================\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "=======================================================\n",
      "AVERAGE\n",
      "=======================================================\n",
      "Logisitc Regression Model:\n",
      "-- Male Accuracy: 0.684\n",
      "-- Female Accuracy: 0.651\n",
      "=======================================================\n",
      "Naive Bayes Model:\n",
      "-- Male Accuracy: 0.672\n",
      "-- Female Accuracy: 0.651\n",
      "=======================================================\n",
      "LGBM Model:\n",
      "-- Male Accuracy: 0.667\n",
      "-- Female Accuracy: 0.662\n",
      "=======================================================\n",
      "=======================================================\n",
      "EPSILON: 0.5\n",
      "=======================================================\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "=======================================================\n",
      "AVERAGE\n",
      "=======================================================\n",
      "Logisitc Regression Model:\n",
      "-- Male Accuracy: 0.687\n",
      "-- Female Accuracy: 0.647\n",
      "=======================================================\n",
      "Naive Bayes Model:\n",
      "-- Male Accuracy: 0.666\n",
      "-- Female Accuracy: 0.647\n",
      "=======================================================\n",
      "LGBM Model:\n",
      "-- Male Accuracy: 0.666\n",
      "-- Female Accuracy: 0.657\n",
      "=======================================================\n",
      "=======================================================\n",
      "EPSILON: 1\n",
      "=======================================================\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "=======================================================\n",
      "AVERAGE\n",
      "=======================================================\n",
      "Logisitc Regression Model:\n",
      "-- Male Accuracy: 0.687\n",
      "-- Female Accuracy: 0.647\n",
      "=======================================================\n",
      "Naive Bayes Model:\n",
      "-- Male Accuracy: 0.664\n",
      "-- Female Accuracy: 0.647\n",
      "=======================================================\n",
      "LGBM Model:\n",
      "-- Male Accuracy: 0.664\n",
      "-- Female Accuracy: 0.648\n",
      "=======================================================\n",
      "=======================================================\n",
      "EPSILON: 2\n",
      "=======================================================\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "=======================================================\n",
      "AVERAGE\n",
      "=======================================================\n",
      "Logisitc Regression Model:\n",
      "-- Male Accuracy: 0.687\n",
      "-- Female Accuracy: 0.646\n",
      "=======================================================\n",
      "Naive Bayes Model:\n",
      "-- Male Accuracy: 0.663\n",
      "-- Female Accuracy: 0.646\n",
      "=======================================================\n",
      "LGBM Model:\n",
      "-- Male Accuracy: 0.66\n",
      "-- Female Accuracy: 0.646\n",
      "=======================================================\n",
      "=======================================================\n",
      "EPSILON: 5\n",
      "=======================================================\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "=======================================================\n",
      "AVERAGE\n",
      "=======================================================\n",
      "Logisitc Regression Model:\n",
      "-- Male Accuracy: 0.687\n",
      "-- Female Accuracy: 0.647\n",
      "=======================================================\n",
      "Naive Bayes Model:\n",
      "-- Male Accuracy: 0.664\n",
      "-- Female Accuracy: 0.647\n",
      "=======================================================\n",
      "LGBM Model:\n",
      "-- Male Accuracy: 0.659\n",
      "-- Female Accuracy: 0.647\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "epsilons = [None, .001, .01, .1, .25, .5, 1, 2, 5] # None gives original accuracy on non-rr data\n",
    "which_set = 'lab' # 'feat-lab', 'feat', 'lab'\n",
    "\n",
    "for e in epsilons:\n",
    "    main(e, which_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdf7f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
